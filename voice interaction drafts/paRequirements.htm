<html dir="ltr" about="" property="dcterms:language" content="en" xmlns="http://www.w3.org/1999/xhtml" prefix="bibo: http://purl.org/ontology/bibo/" typeof="bibo:Document"><head>
        <title>Implied Architecture Requirements for Intelligent Personal Assistants</title>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8">

        <link href="cg-draft.css" rel="stylesheet" type="text/css" charset="utf-8">
    </head>
    <body contenteditable="true"><div class="head">
            <p><a href="http://www.w3.org/">
                    <img width="72" height="48" src="http://www.w3.org/Icons/w3c_home" alt="W3C"></a></p>
            <h1 property="dcterms:title" class="title" id="title">Implied Architecture Requirements</h1>
            <h2 property="bibo:subtitle" id="subtitle">Some basic structure</h2>
            <dl>
                <dt>Latest version</dt>
                <dd><a href="https://github.com/w3c/voiceinteraction">https://github.com/w3c/voiceinteraction</a> (link to this document) </dd>
                <dt>Editor</dt>
                <dd>Deborah Dahl</dd>
                <dd>Conversational Technologies</dd>
            </dl>
            <p class="copyright">Copyright Â© 2023 the Contributors to the Voice Interaction Community Group, 
                published by the  <a href="http://www.w3.org/community/voiceinteraction/">Voice Interaction Community Group</a> 
                under the <a href="https://www.w3.org/community/about/agreements/cla/">W3C Community Contributor License Agreement (CLA)</a>. A human-readable <a href="http://www.w3.org/community/about/agreements/cla-deed/">summary</a> is available.</p>
            <hr></div>

        <h2 id="abstract">Abstract</h2>

        <p>This document was prepared by reviewing version 1.3 of the <a href="https://w3c.github.io/voiceinteraction/voice%20interaction%20drafts/paArchitecture-1-3.htm" >Intelligent Personal Assistant Architecture Report</p> and extracting requirements for the architecture that were implied by the report in order to have a standalone list of requirements. The headings in this document correspond to the headings in the architecture report.

        <h2>Status of This Document</h2>

        <p><em>This specification was published by the 
                <a href="http://www.w3.org/community/voiceinteraction/">Voice Interaction Community Group</a>. 
                It is not a W3C Standard nor is it on the W3C Standards Track. 
                Please note that under the 
                <a href="http://www.w3.org/community/about/agreements/cla/">W3C Community Contributor License Agreement (CLA)</a> there is a limited opt-out and other conditions apply. Learn more about <a href="http://www.w3.org/community/">W3C Community and Business Groups</a>.</em></p>

        <h2 class="introductory">Table of Contents</h2>
        <ul>
            <li>first item</li>
            <li>second item</li>
            <li>Etc.</li>
        </ul>

        <!-- OddPage -->
        <h2><span class="secno">1. </span>Requirements based on Section 1</h2>
        
<ol>
<li>Intelligent Personal Assistants (IPA's) MUST be able to provide general</li> purpose information</li>
<li>Specialized virtual assistants MUST be able to provide enterprise-specific</li> information</li>
<li>Specialized virtual assistants MAY be able to provide non-enterprise-specific</li> information</li>
<li>IPA's SHOULD be able to perform transactions</li>
<li>Specialized assistants MUST be able to interoperate with general IPA's</li>
<li>IPA's SHOULD be able to execute operations in a user's environment</li>
<li>IPA's MUST be able to interact with users through voice or text (language?) or</li> both.</li>
</ol>
<h2><span class="secno">2. </span>Requirements based on Section 2</h2>
<ol>
<li>
IPA's MUST be able to transfer a partially completed task to another IPA
</li>
</ol>
<h2><span class="secno">3. </span>Requirements based on Section 3: Architecture</h2>
<h3>Section 3.1</h3>
<ol>
<li>The architecture SHOULD support question answering and information retrieval</li> applications</li>
<li>The architecture SHOULD support executing local services to accomplish task</li>s</li>
<li>The architecture SHOULD support executing remote services to accomplish task</li>s</li>
<li>The architecture MUST support dynamically adding local and remote services or</li> knowledge sources.</li>
<li>It MUST be possible to forward requests from one IPA to another with the same</li> architecture</li>
<li>It MUST be possible to forward requests or partial requests from one IPA to</li> another with the same architecture, omitting the client layer</li>
<li>IPA extensions MAY be selected from a standardized marketplace </li>
<li>IPA's MAY include a Client layer</li>
<li>IPA's MUST include a Dialog layer</li>
<li>IPA's MAY include an API/Data layer</li>
<li>Components MAY be shifted to other layers as needed (need to clarify with Dirk)?</li>
</ol>

  <h3>Section 3.1</h3>
  <ol>
 <li>	The Client layer MAY include a microphone</li>
<li>	The Client layer MAY include a means for text input</li>
<li>	The Client layer MAY include a speaker</li>
<li>	The Client layer MAY include a display</li>
<li>	Additional (non-speech) output modalities MAY be employed to render output</li> or to capture input

  <h4>Section 3.1.3</h4>
    <ol>
<li>The IPA Client MUST allow activation and deactivation by means of a Client</li> Activation Strategy.
<li>	As an extension IPA Clients MAY also capture input via text and output text.</li>
<li>	As an extension IPA Clients MAY also capture input from various modality</li> recognizers.
<li>	As an extension IPA Clients MAY also capture contextual information, e.g.,</li> location, time, environmental sounds or other inputs that it obtains from Local Data Providers.
<li>	As an extension an IPA Client MAY also receive commands to be executed</li> locally in the Local Services.
<li>	As an extension an IPA Client MAY also receive multimodal output to be</li> rendered by a respective modality synthesizer.
<li>	IPA Clients MAY reference a session identifier.</li>
<li>	Accessibility to be discussed</li>
  <h3>Section 3.2.2.1</h3>
    <ol>
<li>The IPA Client MUST be activated with a Client Activation Strategy</li>
<li>	The Client Activation Strategy MAY be push-to-talk</li>
<li>	The Client Activation Strategy MAY be hotword</li>
<li>	The Client Activation Strategy MAY be triggered by an interpreted text</li> string (either from audio or text)
<li>	The Client Activation Strategy MAY be a change in environment</li>
<li>	The Client Activation Strategy MAY be triggered by a script or environmental</li> condition
<li>	The Client Activation Strategy MAY be a different strategy not enumerated</li> here
  <ol>
<h3>Section 3.2.2.2</h3>
  <ol>
<li>The IPA Client MUST include a Local Service Registry</li>
<li>	The Local Service Registry MUST maintain a list of Local Services</li>
<li>	The Local Service Registry MUST maintain a list of Local Data Providers</li>
  <ol>
<h3>Section 3.2 Dialog Layer</h3>
3.2.1 IPA Service
  <ol>
<li>The IPA Client SHOULD forward audio data and metadata (if any) to the IPA</li> Service
<li>	The IPA Client MAY forward audio data and metadata (if any) to the Dialog</li> Manager
<li>	The IPA Service MUST forward audio data and metadata (if any) to the Dialog</li> Manager
<li>	The IPA Service MUST forward audio data and metadata (if any) to the Local</li> IPA
<li>	The IPA Service MUST forward text data and metadata (if any) to the Dialog</li> Manager
<li>	The IPA Service MUST forward text data and metadata (if any) to the Local IPA</li>
<li>	The IPA Service MUST forward multimodal data and metadata (if any) to the</li> Dialog Manager
<li>	The IPA Service MUST forward multimodal data and metadata (if any) to the</li> Local IPA

<li>	The IPA Service MUST forward audio output from the TTS to the IPA Client</li>
<li>	The IPA Service MUST forward multimodal output from the Dialog Manager to</li> the modality renders
1<li>The IPA Service MUST forward text output from the NLG to the IPA Client</li>

<h3>Section 3.2.2 ASR</h3>
  <ol>
<li>The ASR MUST generate one or more recognition hypotheses from voice input that</li> it receives from the IPA Service
<li>	The ASR MAY associate recognition hypotheses with confidence scores</li>
<li>	The ASR MUST forward the recognition hypotheses to the NLU</li>
<li>	The ASR MAY update the History with the recognition hypotheses</li>

<h3>Section 3.2.3 NLU</h3>
  <ol>
<li>The NLU MUST extract textual interpretations from text strings (either from</li> audio or text)
<li>	The NLU MAY extract multiple interpretations from input text strings (either</li> from audio or text)
<li>	The NLU MUST be able to interpret input Core Intent Sets</li>
<li>	The NLU MUST be able to interpret spoken activation strategies that require</li> interpretation, if they exist
<li>	The NLU MAY make use of the Core Data Provider to access local or internal</li> data or access external services. (revisit Core Data Provider, are we still using that?)
<li>	The NLU MAY make use of the Context to check for complementary information</li> such as information in the history or knowledge
<li>	The NLU MUST forward the semantic interpretation of the input to the Dialog</li> Manager
<li>	The NLU MAY associate statistical confidences with interpretations</li>
<li>	The NLU MAY extract emotion, intention, or sentiment from text strings</li> either from audio or text)
  <ol>

<h3>Section 3.2.4 Dialog Manager</h3>
  <ol>
<li>The Dialog Manager MUST recognize when the user goals are changed</li>
<li>	The Dialog Manager SHOULD confirm when the user goals are changed</li>
<li>	The Dialog Manager MAY consider ongoing workflows that must not be</li> interrupted when the user switches goals.
<li>	The Dialog Manager SHOULD update the History with dialog moves</li>
<li>	The Dialog Manager SHOULD determine the next dialog move </li>
a.	Based on internal considerations
b.	based on output from other components in the same dialog system 
c.	based on output from other agents (IPA services)
d.	how the Dialog Manager determines the next move is outside the scope of these requirements
<li>	??The Dialog Manager makes use of the NLG to generate audio data to be</li> rendered on the IPA Client
 <ol>

<li>	The Dialog Manager MAY provide commands to be executed by the IPA Client or</li> the External Services

<h3>Section 3.2.5 Context</h3>
  <ol>
<li>The Context MAY make use of the Local Service Registry to include external</li> knowledge from Local Data Providers
<li>	The Context MAY make use of the Provider Selection Service to include</li> external knowledge from Data Providers
<li>	The Context MAY provide external knowledge temporarily to the Knowledge</li> Graph to be considered in reasoning.

<h3>Section 3.2.5.1 History</h3>
  <ol>
<li>The Dialog History MAY store the past dialog events per user.</li>

<h3>Section 3.3 API's/Data Layer</h3>
  <ol>
<li>The Provider Selection Service MAY receive input from the Dialog Manager to</li> query data from Data Providers.
<li>The Provider Selection Service MAY receive input from the Dialog Manager to</li> execute External Services.
<li>	If the Provider Selection Service is called by the Dialog Manager with a</li> preselected identifier of an IPA provider, it MUST use the preselected provider
<li>	If the Provider Selection Service is not called with a preselected</li> identifier of an IPA provider, the Provider Selection Service MUST follow a Provider Selection Strategy to determine those IPA Providers that are best suited to answer the request.
  </ol>

        
        <h2><span class="secno">2. </span>Problem Statement</h2>
        
        <h3><span class="secno">2.1 </span>Subsection</h3>

       
        <h4><span class="secno">2.1.1 </span> Now an h4!</h4>

        

        <h2><span class="secno">3. </span>Got style?</h2>

        <ul>
            <li><a href="cg-draft.css">cg-draft.css</a></li>
            <li><a href="cg-final.css">cg-final.css</a></li>
        </ul>

        <p>All of those use <a href="base.css">base.css</a>.</p>

    


</body></html>